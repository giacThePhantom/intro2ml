\chapter{Support vector machines}

\section{Introduzione}
I classificatori lineari presentano due variaiblit\`a principali:
\begin{multicols}{2}
	\begin{itemize}
		\item Quale iperpiano scelgono quando i dati non sono linearmente separabili.
		\item Come gestiscono i dati non linearmente separabili.
	\end{itemize}
\end{multicols}

	\subsection{Perceptron}
	Il perceptron in caso di dati separabili trova un iperpiano che li separa, mentre se non sono separabili continua a modificare i propri valori mentre itera negli esempi.
	L'iperpiano finale dipende pertanto da quali esempi ha visto recentemente.

\section{Support vectors}
Per ogni iperpiano separante esiste un insieme di punti pi\`u vicini chiamati ``closest points'' detti support vectors.
Per $n$ dimensioni ne esistono almeno $n+1$

	\subsection{Large margin classifiers}
	Si dice margine di un classificatore la distanza al punto pi\`u vicino di una classe.
	I large margin classifiers tentano di massimizzare questo aspetto.
	Questa \`e una buona scelta in quanto implica che importano solo i support vectors, mentre gli altri esempi di training sono ignorabili.
	Il margine viene misurato come la distanza dai support vectors.
	Sia $x_i$ un support vector e $w$ il vettore caratteristico dell'iperpiano, allora il margine viene misurato come:
	$$\dfrac{w\cdot x_i+b}{||w||}=\dfrac{1}{||w||}$$
	Per massimizzarlo si seleziona l'iperpiano con il margine pi\`u grande dove i punti sono classificati correttamente al di fuori di esso.
	Si crea pertanto un problema di ottimizzazione con costraint:
	$$max_{x,b} margin(w,b)$$
	Soggetto a:
	$$y_i(w\cdot x_i+b)\ge 1 \forall i$$
	In altre perole:
	$$min_{x,b} ||w||$$
	Soggetto a:
	$$y_i(w\cdot x_i+b)\ge 1 \forall i$$
	Massimizzare il margine \`e equivalente a minimizzare la norma dei pesi soggetti a constraint separati.
	Il constraint assicura che il dato \`e separabile.

	\subsection{Support vector machine problem o soft margin calssification}
	Si vuole pertanto trovare
	$$\min_{w,b} ||w||^2$$
	Soggetto a 
	$$y_i(w\cdot x_i+b)\ge 1 \forall i$$
	Questa \`e una versione di un problema di ottimizzazione quadratico.

